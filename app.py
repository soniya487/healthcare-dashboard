# -*- coding: utf-8 -*-
"""HealthCareProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukxiNn7SApkRxPOKUYlK4K-7OgHP6d_-

Install PySpark in Google Colab
"""

!pip install pyspark

"""ðŸ”¹ Mount Google Drive to Access Datasets"""

from google.colab import drive
drive.mount('/content/drive')

"""Load & Process Big Data in PySpark.
1) Download Sample Healthcare Dataset

--> You can also get a larger dataset like MIMIC-III (Medical ICU Data) from PhysioNet.

"""

!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv -O /content/drive/MyDrive/diabetes.csv

"""ðŸ”¹ Load Dataset with PySpark"""

from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder.appName("HealthcareBigData").getOrCreate()

# Load Dataset
df = spark.read.csv("/content/diabetes.csv", header=False, inferSchema=True)

# Show first 5 records
df.show(100)

""" 3) Feature Engineering & Data Cleaning"""

from pyspark.sql.functions import col

# Rename columns for readability
columns = ["Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]
df = df.toDF(*columns)

# Drop missing values
df = df.na.drop()

# Display cleaned dataset
df.show(100)

"""4) Predictive Healthcare Analytics with ML.
We will train a Random Forest Classifier to predict diabetes risk.

i) Convert Spark DataFrame to Pandas.
Since Scikit-Learn works with Pandas, convert the dataset:

"""

df_pandas = df.toPandas()

"""ii) Train Machine Learning Model"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split data into features (X) and target (y)
X = df_pandas.drop(columns=["Outcome"])  # Features
y = df_pandas["Outcome"]  # Target Variable

# Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Evaluate Model
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f"âœ… Model Accuracy: {accuracy * 100:.2f}%")

from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder.appName("HealthcareBigData").getOrCreate()

# Load Dataset, ensuring header is treated as such
df = spark.read.csv("/content/diabetes.csv", header=True, inferSchema=True)

# ... (rest of the code) ...

# Convert Spark DataFrame to Pandas
df_pandas = df.toPandas()

# Split data into features (X) and target (y)
X = df_pandas.drop(columns=["Outcome"])  # Features
y = df_pandas["Outcome"]  # Target Variable

"""5) Deploy Secure Data Management Using Google BigQuery.

Instead of Hadoop, Google BigQuery can store & query large-scale healthcare data.

i) Set Up Google BigQuery in Colab
"""

!pip install google-cloud-bigquery
from google.cloud import bigquery

# Authenticate Google Cloud
from google.colab import auth
auth.authenticate_user()

"""ii) Uploading Processed Data to BigQuery

Step 1: Ensure Authentication is Set Up
"""

from google.colab import auth
auth.authenticate_user()

"""Step 2: Verify Google Cloud Project ID & Dataset

"""

!gcloud config list project

"""No project ID was set hence we are setting it now by running the below commands."""

!gcloud config set project HealthCareProject

!gcloud auth list
!gcloud config list

!gcloud config set project healthcare-project-12345

!gcloud config list --format="value(core.project)"

!bq mk --dataset healthcare-project-12345.healthcare_dataset

"""Option 1: Use Google Cloudâ€™s Sample Project (Temporary Use)"""

from google.colab import auth
auth.authenticate_user()

import google.auth
project_id = google.auth.default()[1]
print("ðŸ†— Using temporary project ID:", project_id)

dataset_id = f"{project_id}.your_dataset_name"

"""Option 2: Use BigQuery Sandbox (No Billing, No Setup Needed)"""

from google.colab import auth
auth.authenticate_user()
import google.auth
project_id = google.auth.default()[1]
print("Using sandbox project:", project_id)

client = bigquery.Client(project=project_id)
dataset_id = f"{project_id}.your_dataset_name"

!pip install google-cloud-bigquery
from google.cloud import bigquery

# Authenticate Google Cloud
from google.colab import auth
auth.authenticate_user()

!gcloud config list --format="value(core.project)"

!pip install pycryptodome
from Crypto.Cipher import AES
import base64
import os
import pandas as pd

# Use your cleaned dataframe (df_pandas)
def encrypt_column(data, key, iv):
    cipher = AES.new(key, AES.MODE_CBC, iv)
    padded = data + chr(16 - len(data) % 16) * (16 - len(data) % 16)
    encrypted = cipher.encrypt(padded.encode())
    return base64.b64encode(iv + encrypted).decode()

# Generate key and IV
key = os.urandom(32)
iv = os.urandom(16)

# Example: Encrypt only patient diagnosis info
df_pandas['EncryptedOutcome'] = df_pandas['Outcome'].astype(str).apply(lambda x: encrypt_column(x, key, iv))

# Save encrypted data to CSV in Google Drive
df_pandas.to_csv("/content/diabetes.csv", index=False)
print("âœ… Encrypted data saved in Google Drive.")

!pip install streamlit

import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance plot
importances = model.feature_importances_
features = X.columns
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=features)
plt.title("Random Forest Feature Importance")
plt.show()

# Correlation heatmap
# Exclude 'EncryptedOutcome' column before calculating correlation
numerical_df = df_pandas.select_dtypes(include=['number'])  # Select only numerical columns
plt.figure(figsize=(10, 6))
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

from Crypto.Cipher import AES
import base64
import os

def pad(data):
    padding = 16 - len(data) % 16
    return data + chr(padding) * padding

def encrypt(data, key, iv):
    cipher = AES.new(key, AES.MODE_CBC, iv)
    return base64.b64encode(iv + cipher.encrypt(pad(data).encode())).decode()

key = os.urandom(32)
iv = os.urandom(16)

df_pandas['EncryptedOutcome'] = df_pandas['Outcome'].astype(str).apply(lambda x: encrypt(x, key, iv))

def unpad(data):
    return data[:-ord(data[-1])]

def decrypt(enc_data, key):
    raw = base64.b64decode(enc_data)
    iv = raw[:16]
    cipher = AES.new(key, AES.MODE_CBC, iv)
    return unpad(cipher.decrypt(raw[16:]).decode())

# Example
decrypted_values = df_pandas['EncryptedOutcome'].apply(lambda x: decrypt(x, key))
print(decrypted_values.head())

def unpad(data):
    return data[:-ord(data[-1])]

def decrypt(enc_data, key):
    raw = base64.b64decode(enc_data)
    iv = raw[:16]
    cipher = AES.new(key, AES.MODE_CBC, iv)
    return unpad(cipher.decrypt(raw[16:]).decode())

# Example
decrypted_values = df_pandas['EncryptedOutcome'].apply(lambda x: decrypt(x, key))
print(decrypted_values.head())

import google.auth

# Get default credentials and project ID
credentials, project_id = google.auth.default()

print("Project ID:", project_id)

"""Here we are trying to execute the complete project on a website using big data query but there might be a some service issue so as an alternative way we tried to execute the whole by creating google sheets and then connecting to streamlit to run on website"""

from google.cloud import bigquery
from google.colab import auth
import time

auth.authenticate_user()

# Replace with your actual Google Cloud project ID
project_id = 'healthcare-project-12345'
client = bigquery.Client(project=project_id)

# Define dataset and table IDs using the project ID
dataset_id = f"{project_id}.healthcare_dataset"
table_id = f"{dataset_id}.encrypted_diabetes"

job_config = bigquery.LoadJobConfig(
    autodetect=True,
    source_format=bigquery.SourceFormat.CSV,
    write_disposition="WRITE_TRUNCATE",
    skip_leading_rows=1,
)

# Save CSV locally
df_pandas.to_csv("encrypted_diabetes.csv", index=False)

# Retry logic with more verbose output
retries = 3
for attempt in range(retries):
    try:
        with open("encrypted_diabetes.csv", "rb") as source_file:
            job = client.load_table_from_file(source_file, table_id, job_config=job_config)
        job.result()  # Wait for the job to complete
        print("âœ… Data uploaded to BigQuery successfully!")
        break  # Exit loop if successful
    except google.api_core.exceptions.ServiceUnavailable as e:
        print(f"âš ï¸ Service unavailable, retrying ({attempt + 1}/{retries})... Error: {e}")
        time.sleep(5)  # Wait for 5 seconds before retrying
    except Exception as e:  # Catch other potential errors
        print(f"âŒ An error occurred: {e}")
        break  # Exit loop on other errors
    else:
        print("âœ… Data uploaded to BigQuery successfully!")
        break

import time
from google.api_core.exceptions import ServiceUnavailable

retries = 5
delay = 5  # start delay

for attempt in range(retries):
    try:
        with open("encrypted_diabetes.csv", "rb") as source_file:
            job = client.load_table_from_file(source_file, table_id, job_config=job_config)
        job.result()
        print("âœ… Data uploaded to BigQuery successfully!")
        break
    except ServiceUnavailable as e:
        print(f"âš ï¸ Service unavailable. Attempt {attempt + 1}/{retries}. Retrying in {delay}s... Error: {e}")
        time.sleep(delay)
        delay *= 2  # Exponential backoff
    except Exception as e:
        print(f"âŒ Fatal error: {e}")
        break

!pip install --upgrade gspread gspread_dataframe
from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
from gspread_dataframe import set_with_dataframe

creds, _ = default()
gc = gspread.authorize(creds)

# You can rename this if needed
spreadsheet_name = "Encrypted_Diabetes_Data"

try:
    sh = gc.open(spreadsheet_name)
    print("ðŸ“„ Existing sheet found.")
except gspread.exceptions.SpreadsheetNotFound:
    sh = gc.create(spreadsheet_name)
    sh.share('', perm_type='anyone', role='writer')  # Open access for now (can limit later)
    print("ðŸ†• New sheet created.")

# Use the first sheet
worksheet = sh.sheet1

set_with_dataframe(worksheet, df_pandas)
print("âœ… Encrypted data uploaded to Google Sheets successfully!")

print(f"ðŸ”— Open your Google Sheet here: https://docs.google.com/spreadsheets/d/{sh.id}")

import streamlit as st
import gspread
from gspread_dataframe import get_as_dataframe
from google.auth import default
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Authenticate
creds, _ = default()
gc = gspread.authorize(creds)

# Load model
import joblib
joblib.dump(model, "rf_diabetes_model.pkl")

# Now load the model
model = joblib.load("rf_diabetes_model.pkl")

# Open the Google Sheet
sheet_name = "Encrypted_Diabetes_Data"
sh = gc.open(sheet_name)
worksheet = sh.sheet1

# Load data into DataFrame
df = get_as_dataframe(worksheet)

# Clean NaNs or unused columns
df = df.dropna()
if 'EncryptedOutcome' in df.columns:
    df_display = df.drop(columns=['EncryptedOutcome'])  # For analytics
else:
    df_display = df

st.title("ðŸ” Healthcare Risk Dashboard with Google Sheets")

st.subheader("ðŸ“„ Encrypted Data from Google Sheets")
st.dataframe(df_display)

"""Add Risk-Based Patient Segmentation"""

# Add a new column based on predicted risk categories
def categorize_risk(glucose, bmi, age):
    if glucose > 140 or bmi > 30 or age > 50:
        return "High Risk"
    elif glucose > 120 or bmi > 25:
        return "Moderate Risk"
    else:
        return "Low Risk"

df['RiskLevel'] = df.apply(lambda row: categorize_risk(row['Glucose'], row['BMI'], row['Age']), axis=1)

# Countplot
st.subheader("ðŸ§­ Risk Segmentation of Patients")
fig_risk, ax_risk = plt.subplots()
sns.countplot(data=df, x="RiskLevel", palette="coolwarm", ax=ax_risk)
st.pyplot(fig_risk)

"""Correlation Insights for Medical Interpretation"""

!streamlit run app.py

st.subheader("ðŸ“Œ Which Factors Affect Diabetes Most?")
# Exclude 'EncryptedOutcome' and 'RiskLevel' columns before calculating correlation
correlation = df.select_dtypes(include=['number']).drop(columns=['Outcome']).corr()
diabetes_corr = correlation.corrwith(df['Outcome']).sort_values(ascending=False)

st.write(diabetes_corr)

# Heatmap
fig_corr, ax_corr = plt.subplots(figsize=(10,6))
sns.heatmap(df.select_dtypes(include=['number']).drop(columns=['Outcome']).corr(), annot=True, cmap="Reds", ax=ax_corr)
st.pyplot(fig_corr)

